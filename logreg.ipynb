{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "Ya1raxMmVvxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ibm-cos-sdk\n",
        "!pip install ibmos2spark"
      ],
      "metadata": {
        "id": "3BercabEV3nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet"
      ],
      "metadata": {
        "id": "alqQiEI5Vxm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars /usr/local/spark/jars/stocator-1.1.5.jar pyspark-shell\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Parquet File Example\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/spark/jars/stocator-1.1.5.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.stocator.scheme.list\", \"cos\") \\\n",
        "    .config(\"spark.hadoop.fs.cos.impl\", \"com.ibm.stocator.fs.ObjectStoreFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.stocator.cos.impl\", \"com.ibm.stocator.fs.cos.COSAPIClient\") \\\n",
        "    .config(\"spark.hadoop.fs.stocator.cos.scheme\", \"cos\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "7wuvfEXvV8t6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Parquet file\n",
        "try:\n",
        "    df = spark.read.parquet('hmp.parquet')\n",
        "    df.show()\n",
        "except Exception as e:\n",
        "    print(\"Error during reading parquet file:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag3JtejLV_TF",
        "outputId": "304e861b-e4a5-43ed-9d5f-26b9b47d298a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+--------------------+-----------+\n",
            "|  x|  y|  z|              source|      class|\n",
            "+---+---+---+--------------------+-----------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n",
            "+---+---+---+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits = df.randomSplit([0.7, 0.3], seed=42)\n",
        "train_df = splits[0]\n",
        "test_df = splits[1]"
      ],
      "metadata": {
        "id": "6ucfy6-UWPQh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler,Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'class', outputCol='label')\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=['x', 'y', 'z'], outputCol='features')\n",
        "\n",
        "normalizer = Normalizer(inputCol='features', outputCol='norm_features', p=1.0)"
      ],
      "metadata": {
        "id": "6mPw9-c-WRv7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
      ],
      "metadata": {
        "id": "Y2HbvBEMWwis"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, lr])"
      ],
      "metadata": {
        "id": "defqirXPW-tQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(train_df)"
      ],
      "metadata": {
        "id": "17ZUch0BXFXo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.transform(train_df)"
      ],
      "metadata": {
        "id": "LwaPQ2_DXXw0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
        "accuracy = evaluator.evaluate(prediction)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57tksHB4Xglu",
        "outputId": "3553d44b-5fd6-4c08-b986-4a05e65ea37a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.20656587002740848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.transform(test_df)\n",
        "accuracy = evaluator.evaluate(prediction)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mHT2tn6YABJ",
        "outputId": "5baaf1a7-c024-4e2d-c4c8-74d37aa70b05"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.20668767741791277\n"
          ]
        }
      ]
    }
  ]
}